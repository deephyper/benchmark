# FNOBench: Fourier Neural Operator Benchmark

> **Warning**
> Work in progress, this benchmark is not yet ready.

This is a simplified way of using deephyper to conduct the hyperparameter tuning for the TFNO model to solve the darcy flow. The FNO and the TFNO are models for training neural operators and they can be designed using the neural operator package.

The Deep Hyper is a package that automates the design of neural networks using Hyperparameter Tuning and Neural Architecture Search.

## Installation

To use the benchmark follow this example set of instructions:

```console
python -c "import deephyper_benchmark as dhb; dhb.install('FNOBench');"
```

Then load the benchmark from Python:

```python
import deephyper_benchmark as dhb
dhb.load("FNOBench")
from deephyper_benchmark.lib.fnobench import hpo
```

## Configuration

...

## Metadata

The current set of returned metadata is:

- [x] `num_parameters`
- [ ] `num_parameters_train`
- [x] `duration_train`
- [ ] `duration_batch_inference`
- [x] `budget`
- [ ] `stopped`
- [x] `train_loss`: the training loss which is the H1Loss
- [x] `valid_loss`: the validation loss which is the LpLoss
- [ ] `flops`
- [ ] `latency`
- [ ] `lc_train_loss`
- [ ] `lc_valid_loss`

## Other Details

### Training Procedure

We especially edit the Trainer class from the neural operator package to include and return the validation loss and to optimize it.

We also specifically edit the function that loads the data to accommodate the validation data and the test data. Note that the training and testing data file has been included, and is available for download

### Optimized Losses

The losses employed by the neural operator package are the H1Loss and the LpLoss and we stick with those in this benchmark for the sake of consistency.

### Data Splitting Policy


The data we use for this test is the darcy flow test, and we have it in the data folder, it is generated by the load_darcy_flow_small function. The actual function that 
loads and splits the data into train, validation and test data is the load_darcy_pt, and how it achieves this is to load the data from the file 
darcy_train and split it in this form, the train data takes 80% of the data, while the validation data takes 20% and saves it into (x_train, y_train); which is loaded into the trainloader, and (x_valid, y_valid); which is loaded into the validation_loader. Note that we train our model in a particular resolution (16), but we can carry out the test in other resolutions, (for this project we test in 16, 32). To get the test data, we already have the test data in the file darcy_test, and we load it similar to how we load the train data, but here we do not split the test data at all, so we have (x_test, y_test) loaded into the testloaders; testloaderS because we load for resolution 16 and resolution 32.